{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d68074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work 08\n",
    "## Applying written functions to calculate mask properties to all mask output\n",
    "## Applying trained classification model to predict object group (label group) of each mask based on calculated mask properties\n",
    "## Calculating entropy variables (image entropy, whole scene entropy and ground scene entropy)\n",
    "## Summarising all entropy variables for subsequent analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e9f75d8-3be9-45ef-b9ae-ad72fefb7ce1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T13:08:38.317210Z",
     "iopub.status.busy": "2024-02-21T13:08:38.316874Z",
     "iopub.status.idle": "2024-02-21T13:08:46.373129Z",
     "shell.execute_reply": "2024-02-21T13:08:46.372434Z",
     "shell.execute_reply.started": "2024-02-21T13:08:38.317189Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading libraries\n",
    "import torch\n",
    "torch.cuda.empty_cache() \n",
    "import google_streetview\n",
    "import google_streetview.api\n",
    "import google_streetview.helpers\n",
    "import os \n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + \"/configs/\"\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import pyproj\n",
    "from pathlib import Path\n",
    "from scipy.stats import entropy\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "923fac26-61e2-49dd-9099-e31ff4079430",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T13:08:50.163763Z",
     "iopub.status.busy": "2024-02-21T13:08:50.163103Z",
     "iopub.status.idle": "2024-02-21T13:08:50.370345Z",
     "shell.execute_reply": "2024-02-21T13:08:50.369805Z",
     "shell.execute_reply.started": "2024-02-21T13:08:50.163735Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading written functions\n",
    "from mtp_function_yl import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6cdf91",
   "metadata": {},
   "source": [
    "### Mask properties calculation - accident locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b6847be-2042-43a9-b0e6-1a13205a2941",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T13:08:50.371971Z",
     "iopub.status.busy": "2024-02-21T13:08:50.371305Z",
     "iopub.status.idle": "2024-02-21T13:08:50.392373Z",
     "shell.execute_reply": "2024-02-21T13:08:50.391900Z",
     "shell.execute_reply.started": "2024-02-21T13:08:50.371938Z"
    }
   },
   "outputs": [],
   "source": [
    "# loading GSV images\n",
    "koord  = pd.read_csv('/home/yelhe/script/mt/output/koord.csv')\n",
    "name1 = koord.Nr.astype(str) + '1.jpg'\n",
    "name2 = koord.Nr.astype(str) + '2.jpg'\n",
    "name3 = koord.Nr.astype(str) + '3.jpg'\n",
    "name4 = koord.Nr.astype(str) + '4.jpg'\n",
    "gsv_image1 = []\n",
    "gsv_image2 = []\n",
    "gsv_image3 = []\n",
    "gsv_image4 = []\n",
    "nf1 = []\n",
    "nf2 = []\n",
    "nf3 = []\n",
    "nf4 = []\n",
    "for i in range(0,349):\n",
    "    path1 = Path('/home/yelhe/data/gsv/GSV_filtered/' + name1[i])\n",
    "    c1 = path1.is_file()\n",
    "    path2 = Path('/home/yelhe/data/gsv/GSV_filtered/' + name2[i])\n",
    "    c2 = path2.is_file()\n",
    "    path3 = Path('/home/yelhe/data/gsv/GSV_filtered/' + name3[i])\n",
    "    c3 = path3.is_file()\n",
    "    path4 = Path('/home/yelhe/data/gsv/GSV_filtered/' + name4[i])\n",
    "    c4 = path4.is_file()\n",
    "    if c1 is True:\n",
    "        im1 = cv2.imread(\"/home/yelhe/data/gsv/GSV_filtered/\" + name1[i])\n",
    "        im1 = cv2.cvtColor(im1, cv2.COLOR_BGR2RGB)\n",
    "        gsv_image1.append(im1)\n",
    "        nf1.append(name1[i])\n",
    "    if c2 is True:\n",
    "        im2 = cv2.imread(\"/home/yelhe/data/gsv/GSV_filtered/\" + name2[i])\n",
    "        im2 = cv2.cvtColor(im2, cv2.COLOR_BGR2RGB)\n",
    "        gsv_image2.append(im2)\n",
    "        nf2.append(name2[i])\n",
    "    if c3 is True:\n",
    "        im3 = cv2.imread(\"/home/yelhe/data/gsv/GSV_filtered/\" + name3[i])\n",
    "        im3 = cv2.cvtColor(im3, cv2.COLOR_BGR2RGB)\n",
    "        gsv_image3.append(im3)\n",
    "        nf3.append(name3[i])\n",
    "    if c4 is True:\n",
    "        im4 = cv2.imread(\"/home/yelhe/data/gsv/GSV_filtered/\" + name4[i])\n",
    "        im4 = cv2.cvtColor(im4, cv2.COLOR_BGR2RGB)\n",
    "        gsv_image4.append(im4)\n",
    "        nf4.append(name4[i])\n",
    "\n",
    "# loading filtered segmentation output masks \n",
    "n1 = [s.replace('.jpg', '') for s in nf1]\n",
    "n2 = [s.replace('.jpg', '') for s in nf2]\n",
    "n3 = [s.replace('.jpg', '') for s in nf3]\n",
    "n4 = [s.replace('.jpg', '') for s in nf4]\n",
    "m1f = []\n",
    "m2f = []\n",
    "m3f = []\n",
    "m4f = []\n",
    "for i in range(0,295):\n",
    "    m1 = np.load('/home/yelhe/data/gsv_output_filtered_new/' + n1[i] + 'f.npy', allow_pickle=True)\n",
    "    m1f.append(m1)\n",
    "    m2 = np.load('/home/yelhe/data/gsv_output_filtered_new/' + n2[i] + 'f.npy', allow_pickle=True)\n",
    "    m2f.append(m2)\n",
    "    m3 = np.load('/home/yelhe/data/gsv_output_filtered_new/' + n3[i] + 'f.npy', allow_pickle=True)\n",
    "    m3f.append(m3)\n",
    "    m4 = np.load('/home/yelhe/data/gsv_output_filtered_new/' + n4[i] + 'f.npy', allow_pickle=True)\n",
    "    m4f.append(m4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b859cb7-d0ba-484b-93b3-8dc466be49c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-21T13:17:24.591816Z",
     "iopub.status.busy": "2024-02-21T13:17:24.591349Z",
     "iopub.status.idle": "2024-02-21T13:17:24.595037Z",
     "shell.execute_reply": "2024-02-21T13:17:24.594436Z",
     "shell.execute_reply.started": "2024-02-21T13:17:24.591799Z"
    }
   },
   "outputs": [],
   "source": [
    "# applying feature_summary function to calculate mask properties\n",
    "df = pd.DataFrame()\n",
    "df1 = []\n",
    "df2 = []\n",
    "df3 = []\n",
    "df4 = []\n",
    "for i in range(0,295):\n",
    "    df1 = feature_summary(gsv_image1[i], m1f[i])\n",
    "    df1 = df1.assign(Nr = n1[i])\n",
    "    \n",
    "    df2 = feature_summary(gsv_image2[i], m2f[i])\n",
    "    df2 = df2.assign(Nr = n2[i])\n",
    "    \n",
    "    df3 = feature_summary(gsv_image3[i], m3f[i])\n",
    "    df3 = df3.assign(Nr = n3[i])\n",
    "    \n",
    "    df4 = feature_summary(gsv_image4[i], m4f[i])\n",
    "    df4 = df4.assign(Nr = n4[i])\n",
    "    \n",
    "    df = pd.concat([df, df1, df2, df3, df4])\n",
    "    print(i, len(df.index))\n",
    "\n",
    "# saving mask properties of all accident locations\n",
    "df.to_csv('/home/yelhe/script/mt/output/df_accident_0_295.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8fbde8",
   "metadata": {},
   "source": [
    "### Mask properties calculation - random pseudo locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d41ab6b-7fe9-4917-b3b2-d1a5910c4a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading GSV images\n",
    "rp_koord = pd.read_csv('/home/yelhe/script/mt/output/rp_koord_new.csv')\n",
    "rp_koord['id'] = rp_koord.id.astype(str)\n",
    "rp_koord['id'] = rp_koord['id'].str.zfill(4)\n",
    "name1 = rp_koord.id.astype(str) + '1'\n",
    "name2 = rp_koord.id.astype(str) + '2'\n",
    "name3 = rp_koord.id.astype(str) + '3'\n",
    "name4 = rp_koord.id.astype(str) + '4'\n",
    "gsv_imager1 = []\n",
    "gsv_imager2 = []\n",
    "gsv_imager3 = []\n",
    "gsv_imager4 = []\n",
    "nrf1 = []\n",
    "nrf2 = []\n",
    "nrf3 = []\n",
    "nrf4 = []\n",
    "for i in range(0,995):\n",
    "    path1 = Path('/home/yelhe/data/gsv_rp_f/gsv_rp_nf/' + name1[i] + '.jpg')\n",
    "    c1 = path1.is_file()\n",
    "    path2 = Path('/home/yelhe/data/gsv_rp_f/gsv_rp_nf/' + name2[i] + '.jpg')\n",
    "    c2 = path2.is_file()\n",
    "    path3 = Path('/home/yelhe/data/gsv_rp_f/gsv_rp_nf/' + name3[i] + '.jpg')\n",
    "    c3 = path3.is_file()\n",
    "    path4 = Path('/home/yelhe/data/gsv_rp_f/gsv_rp_nf/' + name4[i] + '.jpg')\n",
    "    c4 = path4.is_file()\n",
    "    if c1 is True:\n",
    "        im1 = cv2.imread(\"/home/yelhe/data/gsv_rp_f/gsv_rp_nf/\" + name1[i] + \".jpg\")\n",
    "        gsv_imager1.append(im1)\n",
    "        nrf1.append(name1[i])\n",
    "    if c2 is True:\n",
    "        im2 = cv2.imread(\"/home/yelhe/data/gsv_rp_f/gsv_rp_nf/\" + name2[i] + \".jpg\")\n",
    "        gsv_imager2.append(im2)\n",
    "        nrf2.append(name2[i])\n",
    "    if c3 is True:\n",
    "        im3 = cv2.imread(\"/home/yelhe/data/gsv_rp_f/gsv_rp_nf/\" + name3[i] + \".jpg\")\n",
    "        gsv_imager3.append(im3)\n",
    "        nrf3.append(name3[i])\n",
    "    if c4 is True:\n",
    "        im4 = cv2.imread(\"/home/yelhe/data/gsv_rp_f/gsv_rp_nf/\" + name4[i] + \".jpg\")\n",
    "        gsv_imager4.append(im4)\n",
    "        nrf4.append(name4[i])\n",
    "\n",
    "# loading filtered output mask files\n",
    "m1f = []\n",
    "m2f = []\n",
    "m3f = []\n",
    "m4f = []\n",
    "for i in range(0,792):\n",
    "    m1 = np.load('/home/yelhe/scratch/gsvrp_of/' + nrf1[i] + 'f.npy', allow_pickle=True)\n",
    "    m1f.append(m1)\n",
    "    m2 = np.load('/home/yelhe/scratch/gsvrp_of/' + nrf2[i] + 'f.npy', allow_pickle=True)\n",
    "    m2f.append(m2)\n",
    "    m3 = np.load('/home/yelhe/scratch/gsvrp_of/' + nrf3[i] + 'f.npy', allow_pickle=True)\n",
    "    m3f.append(m3)\n",
    "    m4 = np.load('/home/yelhe/scratch/gsvrp_of/' + nrf4[i] + 'f.npy', allow_pickle=True)\n",
    "    m4f.append(m4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e291d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying written functions to calculate mask properties\n",
    "dfp = pd.DataFrame()\n",
    "dfp1 = []\n",
    "dfp2 = []\n",
    "dfp3 = []\n",
    "dfp4 = []\n",
    "for i in range(0, 792):\n",
    "    dfp1 = feature_summary(gsv_imager1[i], m1f[i])\n",
    "    dfp1 = dfp1.assign(Nr = nrf1[i])\n",
    "    \n",
    "    dfp2 = feature_summary(gsv_imager2[i], m2f[i])\n",
    "    dfp2 = dfp2.assign(Nr = nrf2[i])\n",
    "    \n",
    "    dfp3 = feature_summary(gsv_imager3[i], m3f[i])\n",
    "    dfp3 = dfp3.assign(Nr = nrf3[i])\n",
    "    \n",
    "    dfp4 = feature_summary(gsv_imager4[i], m4f[i])\n",
    "    dfp4 = dfp4.assign(Nr = nrf4[i])\n",
    "    \n",
    "    dfp = pd.concat([dfp, dfp1, dfp2, dfp3, dfp4])\n",
    "    print(i, len(dfp.index))    \n",
    "# saving mask properties of all random pseudo locations    \n",
    "dfp.to_csv('/home/yelhe/scratch/df/df_pseudo_0_792_new.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6e75e",
   "metadata": {},
   "source": [
    "### Prediction of object (label) group of masks - curb extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded97065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading files of calculated mask properites\n",
    "df_a = pd.read_csv('/home/yelhe/scratch/df/df_accident_0_295.csv')\n",
    "df_p = pd.read_csv('/home/yelhe/scratch/df/df_pseudo_0_792_new.csv')\n",
    "# preparing for classification model\n",
    "dfa = df_a\n",
    "dfp = df_p\n",
    "dfa = dfa.drop(['mask', 'Nr','isconvex','is_cen_inside','is_mce_inside', 'leftm', 'rightm'], axis = 1)\n",
    "dfp = dfp.drop(['mask', 'Nr','isconvex','is_cen_inside','is_mce_inside', 'leftm', 'rightm'], axis = 1)\n",
    "# selecting only the masks located in the lower part\n",
    "dfa = dfa.loc[dfa['topm'] >= 300]\n",
    "dfp = dfp.loc[dfp['topm'] >= 300]\n",
    "dfa = dfa.drop(['topm'], axis = 1)\n",
    "dfp = dfp.drop(['topm'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading trained RF classification model for curb extraction\n",
    "rf = joblib.load(\"my_random_forest_1.joblib\")\n",
    "y_pred_a = rf.predict(dfa)\n",
    "y_pred_p = rf.predict(dfp)\n",
    "dfal = dfa\n",
    "dfal['p'] = y_pred_a\n",
    "dfpl = dfp\n",
    "dfpl['p'] = y_pred_p\n",
    "dfal.to_csv('/home/yelhe/script/mt/output/pred_accident.csv', index = False)\n",
    "dfpl.to_csv('/home/yelhe/script/mt/output/pred_pseudo.csv', index = False)\n",
    "dfa0 = df_a\n",
    "dfp0 = df_p\n",
    "dfa0 = dfa0.loc[dfa0['topm'] >= 300]\n",
    "dfp0 = dfp0.loc[dfp0['topm'] >= 300]\n",
    "dfa_pl = pd.merge(dfa0, dfal, on = ['gmedian', 'rmedian', 'bmedian', 'gmean', \n",
    "                                   'rmean', 'bmean', 'gstd', 'rstd', 'bstd', \n",
    "                                   'gq25', 'gq75', 'rq25', 'rq75', 'bq25', \n",
    "                                   'bq75', 'cdmean', 'cdstd', 'area', 'aspect_ratio_wh_s', \n",
    "                                   'extent_s', 'solidity', 'aspect_ratio_wh', 'extent', \n",
    "                                   'orien_rre', 'orien_ell', 'ed', 'ratio_ell', 'perimeter', 'bottomm'])\n",
    "dfp_pl = pd.merge(dfp0, dfpl, on = ['gmedian', 'rmedian', 'bmedian', 'gmean', \n",
    "                                   'rmean', 'bmean', 'gstd', 'rstd', 'bstd', \n",
    "                                   'gq25', 'gq75', 'rq25', 'rq75', 'bq25', \n",
    "                                   'bq75', 'cdmean', 'cdstd', 'area', 'aspect_ratio_wh_s', \n",
    "                                   'extent_s', 'solidity', 'aspect_ratio_wh', 'extent', \n",
    "                                   'orien_rre', 'orien_ell', 'ed', 'ratio_ell', 'perimeter', 'bottomm'])\n",
    "\n",
    "# saving predicted label of masks into files\n",
    "dfa_pl.to_csv('/home/yelhe/script/mt/output/pred_accident_label.csv', index = False)\n",
    "dfp_pl.to_csv('/home/yelhe/script/mt/output/pred_pseudo_label.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665fa483",
   "metadata": {},
   "source": [
    "### Prediction of object (label) group of masks - whole scene entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473feba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing\n",
    "dfae = df_a\n",
    "dfpe = df_p\n",
    "dfae = dfae.drop(['mask', 'Nr','isconvex','is_cen_inside','is_mce_inside'], axis = 1)\n",
    "dfpe = dfpe.drop(['mask', 'Nr','isconvex','is_cen_inside','is_mce_inside'], axis = 1)\n",
    "# loading trained RF classification model for whole scene entropy\n",
    "rfe = joblib.load(\"my_random_forest_2_whole7.joblib\")\n",
    "y_pred_aw = rfe.predict(dfae)\n",
    "print(np.unique(y_pred_aw, return_counts=True))\n",
    "y_pred_pw = rfe.predict(dfpe)\n",
    "print(np.unique(y_pred_pw, return_counts=True))\n",
    "# saving into dataframes\n",
    "dfael = dfae\n",
    "dfael['p'] = y_pred_aw\n",
    "dfpel = dfpe\n",
    "dfpel['p'] = y_pred_pw\n",
    "dfa1 = df_a\n",
    "dfp1 = df_p\n",
    "# dataframe_accident_forentropy_predicted_label\n",
    "dfa_epl = pd.merge(dfa1, dfael, on = ['gmedian', 'rmedian', 'bmedian', 'gmean', \n",
    "                                   'rmean', 'bmean', 'gstd', 'rstd', 'bstd', \n",
    "                                   'gq25', 'gq75', 'rq25', 'rq75', 'bq25', \n",
    "                                   'bq75', 'cdmean', 'cdstd', 'area', 'aspect_ratio_wh_s', \n",
    "                                   'extent_s', 'solidity', 'aspect_ratio_wh', 'extent', \n",
    "                                   'orien_rre', 'orien_ell', 'ed', 'ratio_ell', 'perimeter', 'bottomm',\n",
    "                                   'topm', 'leftm', 'rightm'])\n",
    "# dataframe_pseudo_forentropy_predicted_label\n",
    "dfp_epl = pd.merge(dfp1, dfpel, on = ['gmedian', 'rmedian', 'bmedian', 'gmean', \n",
    "                                   'rmean', 'bmean', 'gstd', 'rstd', 'bstd', \n",
    "                                   'gq25', 'gq75', 'rq25', 'rq75', 'bq25', \n",
    "                                   'bq75', 'cdmean', 'cdstd', 'area', 'aspect_ratio_wh_s', \n",
    "                                   'extent_s', 'solidity', 'aspect_ratio_wh', 'extent', \n",
    "                                   'orien_rre', 'orien_ell', 'ed', 'ratio_ell', 'perimeter', 'bottomm',\n",
    "                                   'topm', 'leftm', 'rightm'])\n",
    "dfa_epl.groupby(['p']).size().reset_index().rename(columns={0:'count'})\n",
    "# p count \n",
    "# 0 5957\n",
    "# 1 316\n",
    "# 2 9490\n",
    "# 3 1655\n",
    "# 4 30879\n",
    "# 5 1798\n",
    "# 6 7239"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719c09fb",
   "metadata": {},
   "source": [
    "### Prediction of object (label) group of masks - ground scene entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dd8791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing (using the infrastructure groups recognized by model above)\n",
    "dfai = dfa_epl\n",
    "dfpi = dfp_epl\n",
    "# label: 1 curb, 2 infrastructure\n",
    "dfai = dfa_epl.loc[(dfa_epl['p'] == 1)|(dfa_epl['p'] == 2)]\n",
    "dfpi = dfp_epl.loc[(dfp_epl['p'] == 1)|(dfp_epl['p'] == 2)]\n",
    "# loading trained classification model for ground scene entropy\n",
    "rfi = joblib.load(\"my_random_forest_3_ground7_new1.joblib\")\n",
    "# preparing for prediction\n",
    "dfai = dfai.drop(['mask', 'Nr','isconvex','is_cen_inside','is_mce_inside', 'p'], axis = 1)\n",
    "dfpi = dfpi.drop(['mask', 'Nr','isconvex','is_cen_inside','is_mce_inside', 'p'], axis = 1)\n",
    "y_pred_ai = rfi.predict(dfai)\n",
    "print(np.unique(y_pred_ai, return_counts=True))\n",
    "y_pred_pi = rfi.predict(dfpi)\n",
    "print(np.unique(y_pred_pi, return_counts=True))\n",
    "# saving into dataframes\n",
    "dfail = dfai\n",
    "dfail['p'] = y_pred_ai\n",
    "dfpil = dfpi\n",
    "dfpil['p'] = y_pred_pi\n",
    "dfa1 = df_a\n",
    "dfp1 = df_p\n",
    "# dataframe_accident_forentropy_infrastructure_ground_predicted_label\n",
    "dfa_eipl = pd.merge(dfa1, dfail, on = ['gmedian', 'rmedian', 'bmedian', 'gmean', \n",
    "                                   'rmean', 'bmean', 'gstd', 'rstd', 'bstd', \n",
    "                                   'gq25', 'gq75', 'rq25', 'rq75', 'bq25', \n",
    "                                   'bq75', 'cdmean', 'cdstd', 'area', 'aspect_ratio_wh_s', \n",
    "                                   'extent_s', 'solidity', 'aspect_ratio_wh', 'extent', \n",
    "                                   'orien_rre', 'orien_ell', 'ed', 'ratio_ell', 'perimeter', 'bottomm',\n",
    "                                   'topm', 'leftm', 'rightm'])\n",
    "# dataframe_pseudo_forentropy_infrastructure_ground_predicted_label\n",
    "dfp_eipl = pd.merge(dfp1, dfpil, on = ['gmedian', 'rmedian', 'bmedian', 'gmean', \n",
    "                                   'rmean', 'bmean', 'gstd', 'rstd', 'bstd', \n",
    "                                   'gq25', 'gq75', 'rq25', 'rq75', 'bq25', \n",
    "                                   'bq75', 'cdmean', 'cdstd', 'area', 'aspect_ratio_wh_s', \n",
    "                                   'extent_s', 'solidity', 'aspect_ratio_wh', 'extent', \n",
    "                                   'orien_rre', 'orien_ell', 'ed', 'ratio_ell', 'perimeter', 'bottomm',\n",
    "                                   'topm', 'leftm', 'rightm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4b5ac1",
   "metadata": {},
   "source": [
    "### Calculation of entropy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d5535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating whole scene entropy\n",
    "awe = dfa_epl[['Nr', 'p']]\n",
    "pwe = dfp_epl[['Nr', 'p']]\n",
    "awe.loc[:,'Nr'] = awe.loc[:,'Nr'].values.astype(str)\n",
    "awe.loc[:,'id'] = awe.Nr.str[-15:-1]\n",
    "awe.loc[:,'dir'] = awe.Nr.str[-1]\n",
    "pwe.loc[:,'Nr'] = pwe.loc[:,'Nr'].values.astype(str)\n",
    "pwe.loc[:,'Nr'] = pwe.loc[:,'Nr'].str.zfill(5)\n",
    "pwe.loc[:,'id'] = pwe.Nr.str[-5:-1]\n",
    "pwe.loc[:,'dir'] = pwe.Nr.str[-1]\n",
    "# predicted curb in accident points summary\n",
    "awes = awe.groupby(['id', 'dir', 'p']).size().reset_index().rename(columns={0:'count'})\n",
    "# predicted curb in pseudo points summary\n",
    "pwes = pwe.groupby(['id', 'dir','p']).size().reset_index().rename(columns={0:'count'})\n",
    "# applying written functions to calculate whole scene entropy\n",
    "dfentropya = entropy_calculate(awes)\n",
    "dfentropyp = entropy_calculate(pwes)\n",
    "dfentropya.to_csv('/home/yelhe/script/mt/output/accident_predmaskgroup_entropy.csv', index = False)\n",
    "dfentropyp.to_csv('/home/yelhe/script/mt/output/pseudo_predmaskgroup_entropy.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c982a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating ground scene entropy\n",
    "aie = dfa_eipl.loc[:, ['Nr','p']]\n",
    "pie = dfp_eipl.loc[:, ['Nr','p']]\n",
    "aie.loc[:,'Nr'] = aie.loc[:,'Nr'].values.astype(str)\n",
    "aie.loc[:,'id'] = aie.Nr.str[-15:-1]\n",
    "aie.loc[:,'dir'] = aie.Nr.str[-1]\n",
    "pie.loc[:,'Nr'] = pie.loc[:,'Nr'].values.astype(str)\n",
    "pie.loc[:,'Nr'] = pie.loc[:,'Nr'].str.zfill(5)\n",
    "pie.loc[:,'id'] = pie.Nr.str[-5:-1]\n",
    "pie.loc[:,'dir'] = pie.Nr.str[-1]\n",
    "# predicted curb in accident points summary in ground_infrastructure scene\n",
    "aies = aie.groupby(['id', 'dir', 'p']).size().reset_index().rename(columns={0:'count'})\n",
    "# predicted curb in pseudo points summary in ground_infrastructure scene\n",
    "pies = pie.groupby(['id', 'dir', 'p']).size().reset_index().rename(columns={0:'count'})\n",
    "# applying written functions to calculate ground scene entropy\n",
    "dfentropya_i = entropy_calculate(aies)\n",
    "dfentropyp_i = entropy_calculate(pies)\n",
    "dfentropya_i.to_csv('/home/yelhe/script/mt/output/accident_predmaskgroup_gi_entropy_new.csv', index = False)\n",
    "dfentropyp_i.to_csv('/home/yelhe/script/mt/output/pseudo_predmaskgroup_gi_entropy_new.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0486579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating image entropy\n",
    "# applying written function to calculate image entropy for accident locations\n",
    "en1 = []\n",
    "en2 = []\n",
    "en3 = []\n",
    "en4 = []\n",
    "for i in range(0,295):\n",
    "    e1 = entropy_img(gsv_image1[i])\n",
    "    en1.append(e1)\n",
    "    e2 = entropy_img(gsv_image2[i])\n",
    "    en2.append(e2)\n",
    "    e3 = entropy_img(gsv_image3[i])\n",
    "    en3.append(e3)\n",
    "    e4 = entropy_img(gsv_image4[i])\n",
    "    en4.append(e4)\n",
    "dfe1 = pd.DataFrame()\n",
    "dfe2 = pd.DataFrame()\n",
    "dfe3 = pd.DataFrame()\n",
    "dfe4 = pd.DataFrame()\n",
    "dfe1 = dfe1.assign(Nr = n1)\n",
    "dfe1 = dfe1.assign(entropy = en1)\n",
    "dfe2 = dfe2.assign(Nr = n2)\n",
    "dfe2 = dfe2.assign(entropy = en2)\n",
    "dfe3 = dfe3.assign(Nr = n3)\n",
    "dfe3 = dfe3.assign(entropy = en3)\n",
    "dfe4 = dfe4.assign(Nr = n4)\n",
    "dfe4 = dfe4.assign(entropy = en4)\n",
    "dfe = pd.concat([dfe1, dfe2, dfe3, dfe4])\n",
    "dfe.loc[:,'Nr'] = dfe.loc[:,'Nr'].values.astype(str)\n",
    "dfe.loc[:,'id'] = dfe.Nr.str[-15:-1]\n",
    "dfe.loc[:,'dir'] = dfe.Nr.str[-1]\n",
    "dfe.to_csv('/home/yelhe/script/mt/output/accident_image_entropy.csv', index = False)\n",
    "\n",
    "# applying written function to calculate image entropy for random pseudo locations\n",
    "enr1 = []\n",
    "enr2 = []\n",
    "enr3 = []\n",
    "enr4 = []\n",
    "for i in range(0,792):\n",
    "    e1 = entropy_img(gsv_imager1[i])\n",
    "    enr1.append(e1)\n",
    "    e2 = entropy_img(gsv_imager2[i])\n",
    "    enr2.append(e2)\n",
    "    e3 = entropy_img(gsv_imager3[i])\n",
    "    enr3.append(e3)\n",
    "    e4 = entropy_img(gsv_imager4[i])\n",
    "    enr4.append(e4)\n",
    "dfer1 = pd.DataFrame()\n",
    "dfer2 = pd.DataFrame()\n",
    "dfer3 = pd.DataFrame()\n",
    "dfer4 = pd.DataFrame()\n",
    "dfer1 = dfer1.assign(Nr = nrf1)\n",
    "dfer1 = dfer1.assign(entropy = enr1)\n",
    "dfer2 = dfer2.assign(Nr = nrf2)\n",
    "dfer2 = dfer2.assign(entropy = enr2)\n",
    "dfer3 = dfer3.assign(Nr = nrf3)\n",
    "dfer3 = dfer3.assign(entropy = enr3)\n",
    "dfer4 = dfer4.assign(Nr = nrf4)\n",
    "dfer4 = dfer4.assign(entropy = enr4)\n",
    "dfer = pd.concat([dfer1, dfer2, dfer3, dfer4])\n",
    "dfer.loc[:,'Nr'] = dfer.loc[:,'Nr'].values.astype(str)\n",
    "dfer.loc[:,'id'] = dfer.Nr.str[-5:-1]\n",
    "dfer.loc[:,'dir'] = dfer.Nr.str[-1]\n",
    "dfer.to_csv('/home/yelhe/script/mt/output/pseudo_image_entropy.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668c19f",
   "metadata": {},
   "source": [
    "### Summary of entropy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2f32f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading files of calculated entropy \n",
    "# image entropy\n",
    "iea = pd.read_csv('/home/yelhe/script/mt/output/accident_image_entropy.csv')\n",
    "iep = pd.read_csv('/home/yelhe/script/mt/output/pseudo_image_entropy.csv')\n",
    "# mask entropy of whole scene\n",
    "dfmwa = pd.read_csv('/home/yelhe/script/mt/output/accident_predmaskgroup_entropy.csv')\n",
    "dfmwp = pd.read_csv('/home/yelhe/script/mt/output/pseudo_predmaskgroup_entropy.csv')\n",
    "# mask entropy of ground scene\n",
    "dfmga = pd.read_csv('/home/yelhe/script/mt/output/accident_predmaskgroup_gi_entropy_new.csv')\n",
    "dfmgp = pd.read_csv('/home/yelhe/script/mt/output/pseudo_predmaskgroup_gi_entropy_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc75e683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accident locations\n",
    "# for image entropy\n",
    "iea['Nr'] = iea['Nr'].values.astype(str)\n",
    "iea['id'] = iea['id'].values.astype(str)\n",
    "iea.rename(columns = {'entropy':'ie'}, inplace = True)\n",
    "# for whole scene entropy\n",
    "mwa = dfmwa.drop(['p', 'count', 'sum', 'percentage'], axis = 1)\n",
    "mwa = mwa.drop_duplicates()\n",
    "mwa['id'] = mwa['id'].values.astype(str)\n",
    "mwa.rename(columns = {'entropy':'mew', 'id_dir':'Nr'}, inplace = True)\n",
    "# for ground scene entropy\n",
    "mga = dfmga.drop(['p', 'count', 'sum', 'percentage'], axis = 1)\n",
    "mga = mga.drop_duplicates()\n",
    "mga['id'] = mga['id'].values.astype(str)\n",
    "mga.rename(columns = {'entropy':'meg', 'id_dir':'Nr'}, inplace = True)\n",
    "# summarising\n",
    "df_me = pd.merge(mwa, mga, on = ['id', 'dir', 'Nr'])\n",
    "df_me['Nr'] = df_me['Nr'].values.astype(str)\n",
    "df_me['id'] = df_me['id'].values.astype(str)\n",
    "df_me['dir'] = df_me['dir'].values.astype(str)\n",
    "iea['Nr'] = iea['Nr'].values.astype(str)\n",
    "iea['id'] = iea['id'].values.astype(str)\n",
    "iea['dir'] = iea['dir'].values.astype(str)\n",
    "df_en_a = pd.merge(df_me, iea, on = ['id', 'dir', 'Nr'])\n",
    "# saving into file\n",
    "df_en_a.to_csv('/home/yelhe/script/mt/output/accident_entropy_sum_new.csv', index = False)\n",
    "\n",
    "# random pseudo locations\n",
    "# for image scene entropy\n",
    "iep['Nr'] = iep['Nr'].values.astype(str)\n",
    "iep['Nr'] = iep['Nr'].str.zfill(5)\n",
    "iep['id'] = iep['id'].values.astype(str)\n",
    "iep['id'] = iep['id'].str.zfill(4)\n",
    "iep.rename(columns = {'entropy':'ie'}, inplace = True)\n",
    "# for whole scene entropy\n",
    "mwp = dfmwp.drop(['p', 'count', 'sum', 'percentage'], axis = 1)\n",
    "mwp = mwp.drop_duplicates()\n",
    "mwp.rename(columns = {'entropy':'mew', 'id_dir':'Nr'}, inplace = True)\n",
    "mwp['Nr'] = mwp['Nr'].values.astype(str)\n",
    "mwp['Nr'] = mwp['Nr'].str.zfill(5)\n",
    "mwp['id'] = mwp['id'].values.astype(str)\n",
    "mwp['id'] = mwp['id'].str.zfill(4)\n",
    "# for ground scene entropy\n",
    "mgp = dfmgp.drop(['p', 'count', 'sum', 'percentage'], axis = 1)\n",
    "mgp = mgp.drop_duplicates()\n",
    "mgp.rename(columns = {'entropy':'meg', 'id_dir':'Nr'}, inplace = True)\n",
    "mgp['Nr'] = mgp['Nr'].values.astype(str)\n",
    "mgp['Nr'] = mgp['Nr'].str.zfill(5)\n",
    "mgp['id'] = mgp['id'].values.astype(str)\n",
    "mgp['id'] = mgp['id'].str.zfill(4)\n",
    "# summarising\n",
    "df_me_p = pd.merge(mwp, mgp, on = ['id', 'dir', 'Nr'])\n",
    "df_en_p = pd.merge(df_me_p, iep, on = ['id', 'dir', 'Nr'])\n",
    "# saving into file\n",
    "df_en_p.to_csv('/home/yelhe/script/mt/output/pseudo_entropy_sum_new.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
